Технічний гайд: інтеграція n8n, LLM-агентів та відлагодження

1. Інтеграція кастомного розширення з n8n

Способи інтеграції кастомного модуля з n8n:
	•	Webhook (вхідні HTTP-запити): Нода Webhook в n8n дозволяє створити HTTP endpoint, який спрацьовує при отриманні запиту. Webhook-нода є тригером, що може запускати workflow при надходженні даних зі сторонніх сервісів ￼. Вона підтримує різні методи (GET, POST тощо) та навіть може повертати відповідь із кінцевих даних workflow, фактично даючи можливість будувати власні API на базі n8n ￼ ￼. Такий підхід зручний, якщо кастомне розширення вміє надсилати вебхуки – просто спрямовуєте їх на URL, згенерований Webhook-нодою, щоб ініціювати процес в n8n.
	•	HTTP Request (вихідні виклики HTTP): Для активної інтеграції, коли потрібно з n8n звертатися до API вашого розширення, використовується нода HTTP Request. Вона дозволяє виконувати запити до зовнішніх REST API прямо зі сценарію. Як зазначають розробники n8n, HTTP Request – “це саме та нода, яка вам потрібна” для інтеграції з будь-яким зовнішнім веб-сервісом ￼. Ви можете налаштувати метод (GET, POST, PUT тощо), тіло запиту (JSON тощо) і заголовки. n8n підтримує різні види автентифікації в HTTP Request (Basic, OAuth2, API Key та ін.), що спрощує підключення до захищених API. Це підходящий спосіб, якщо ваше кастомне розширення надає HTTP API – ви просто викликаєте його ендпоінти з n8n.
	•	Кастомна нода (розширення n8n): n8n – модульна платформа, тому ви можете написати власну ноду під специфіку свого розширення. Для цього потрібно знання JavaScript/TypeScript: ви створюєте npm-пакет з реалізацією ноди і встановлюєте його в n8n. Офіційна документація містить посібники зі створення custom nodes та їх тестування ￼. Фактично, n8n надає можливість “build custom nodes, install them on your n8n instance, and publish them to npm” ￼. Цей шлях більш трудомісткий, але дає найбільшу гнучкість – ви інтегруєте свою логіку безпосередньо в інтерфейс n8n як повноцінну ноду із параметрами, що зручно використовувати у будь-яких workflow.
	•	API n8n (публічний API): Якщо необхідно керувати n8n ззовні (наприклад, запускати workflow програмно), можна скористатися публічним REST API самої n8n ￼. Через нього можна виконувати багато дій, доступних у GUI, але вже з коду – наприклад, запускати певний workflow, передавати йому дані, отримувати результати тощо. n8n навіть має спеціальну ноду n8n API, щоб викликати свій API із середини workflow ￼. Проте важливо: прямого публічного ендпоінту для виконання довільного workflow наразі нема (він є лише як внутрішній приватний /rest/workflows/run) ￼ ￼. Розробники рекомендують не покладатися на внутрішні невідомі ендпоінти, а реалізувати запуск через Webhook-тригер та ноду Execute Workflow ￼. Тобто, якщо вам потрібно стартувати процес через API – створіть у n8n webhook, що викликає потрібний workflow (або запускає підпроцес через Execute Workflow). Таким чином кастомне розширення може просто зробити HTTP-запит на webhook n8n замість прямого виклику внутрішнього API.

Порівняння підходів інтеграції:

Підхід	Опис і використання	Коли застосовувати
Webhook	Отримує HTTP-запити для запуску workflow в n8n ￼. Повертає відповідь по завершенні (режим API-ендпоінту) ￼.	Коли зовнішній додаток (розширення) може надсилати вебхуки самостійно.
HTTP Request	Виконує HTTP-запит з n8n до зовнішнього API (REST/JSON). Налаштовуються метод, URL, тіло, автентифікація тощо ￼.	Коли треба з workflow звернутися до функцій або даних розширення через його API.
Кастомна нода	Власноруч написаний модуль-нода для n8n (JS/TS). Інсталюється як плагін, з’являється в інтерфейсі n8n ￼.	Коли потрібна тісна інтеграція, повторне використання та гнучкість, і ви готові писати код ноди.
Публічний API n8n	Використання REST API самої n8n (через запити або через вбудовану ноду n8n API) для запуску або керування воркфловами ￼.	Коли треба запускати або контролювати workflow програмно ззовні (напряму або опосередковано). Зазвичай замінюється на Webhook-тригер для практичності.

2. Підключення власної CRM через REST API

Якщо ви маєте самописну CRM-систему, яка надає REST API, інтегрувати її з n8n можна за допомогою стандартних інструментів або кастомних інтеграцій.

Через стандартні засоби (HTTP Request): Найпростіший шлях – використовувати ноду HTTP Request для звернення до REST API вашої CRM. Ви задаєте URL ендпоінтів CRM, метод запиту (GET для отримання даних, POST/PUT для створення чи оновлення), а також передаєте потрібні дані (тіло запиту у форматі JSON) і параметри автентифікації. Наприклад, щоб передати лід з n8n до CRM, ви можете налаштувати HTTP Request з методом POST на ендпоінт типу /Leads/NovaLead і в тілі (Body) сформувати JSON з полями ліда ￼ ￼. Якщо CRM потребує автентифікації (токен або логін), n8n дозволяє налаштувати це: у властивостях HTTP Request є опція Authentication, де можна обрати тип (“Generic Credential” для нестандартних API). Далі в залежності від вимог API задаються поля – наприклад, для Basic Auth можна ввести логін і пароль, для Bearer Token – обрати “Header Auth” і прописати заголовок Authorization з токеном ￼. У випадку OAuth2 API теж підтримуються (через OAuth2 Credential).

Приклад налаштування з двоетапною авторизацією: Якщо API CRM спочатку видає токен за логіном/паролем, а потім цей токен треба використовувати в усіх запитах – це реалізується двома HTTP нодами. Перша HTTP Request надсилає POST запит на ендпоінт отримання токена (/api/Acesso/Token), з потрібними параметрами (логін, пароль, grant_type тощо) ￼. У ноді можна встановити Auth тип Basic і передати креденшіали, або просто в Body – залежно від API. Відповіддю буде access_token. Друга HTTP Request-нода викликає вже основний ендпоінт CRM (наприклад, створення ліда) – у ній в полі Authentication обирається тип Header Auth, де ім’я заголовка Authorization, а значення Bearer <отриманий токен> ￼ ￼. Таким чином, n8n спочатку авторизується, потім виконує дію з використанням токена. Такий підхід (отримати токен, потім використовувати) підтверджено в спільноті n8n як робочий для інтеграції з захищеними API ￼.

Кастомні інтеграції для CRM: Якщо ваша CRM дуже специфічна, можна створити кастомну ноду для неї (як описано в пункті 1). Проте часто це не потрібно, адже HTTP Request покриває більшість випадків. Альтернативно, якщо CRM може надсилати вебхуки про певні події (напр., новий лід), має сенс налаштувати Webhook-ноду в n8n, щоб приймати ці події. Наприклад, при створенні запису в CRM вона б’є по URL n8n (Webhook) і запускає обробку – таким чином, CRM “пушить” дані в n8n. У зворотному напрямку, для “витягування” даних, є Polling-підходи: запускати workflow за розкладом (через Cron) і робити HTTP GET до API CRM для отримання нових даних.

Налаштування креденшіалів: Бажано винести секрети (ключі API, логін/пароль) у Credentials в n8n. Для нестандартної CRM підійде тип Generic API або HTTP Basic в менеджері Credentials. Це дозволяє безпечно зберегти доступ і перевикористовувати його в кількох нодах. У HTTP Request ноді потім достатньо вибрати збережений креденшл, щоб не дублювати секретні дані.

Стандартні vs кастомні інтеграції: Підсумовуючи, стандартний спосіб – через HTTP Request – зазвичай достатній: “просто оберіть метод POST і надішліть потрібні дані, додавши їх як вміст тіла” ￼. Кастомна інтеграція (на рівні коду) знадобиться лише якщо потрібна глибока інтеграція або складна логіка, яку важко реалізувати комбінацією стандартних нод. У більшості випадків ланцюжок з декількох нод (HTTP для токену, HTTP для основного запиту, плюс, можливо, Nodes для обробки відповіді) успішно підключить вашу CRM до n8n.

3. Deep Search API: документація та приклади

Deep Search API – це AI-пошуковий сервіс від Jina AI, який поєднує пошук в інтернеті, читання знайдених джерел та логічні висновки. В контексті інтеграції з n8n він цікавий тим, що надає готовий API для “глибокого” пошуку інформації, і при цьому сумісний за схемою із API OpenAI. Це означає, що для використання Deep Search можна застосовувати ті ж підходи, що й до ChatGPT API.

Авторизація та доступ: Deep Search API безкоштовний для використання, ключ API не обов’язковий. Якщо ви не передаєте API-ключ, сервіс працюватиме на базових лімітах запитів. Водночас, при наданні API-ключа (який можна отримати зареєструвавшись на сайті Jina AI), ви отримаєте вищий ліміт запитів; при цьому сам ключ не тарифікується (тобто Jina не стягує плату, поки не перевищено безкоштовний ліміт) ￼. Отриманий API-ключ передається як Bearer Token у заголовку Authorization (стандартний підхід). Якщо ключа нема, можна робити запити і без нього – просто з більш суворим лімітом.

Ендпоінти та модель: Базовий URL для запитів – https://deepsearch.jina.ai. Як і у OpenAI, використовується версія API v1. Основний ендпоінт – /v1/chat/completions, який працює за схемою ChatCompletions OpenAI. Тобто, ви надсилаєте JSON, що містить поле model і масив messages (діалог користувача з асистентом). Вказувати потрібно модель jina-deepsearch-v1 – це ідентифікатор моделі Deep Search ￼. Всі інші параметри підтримуються аналогічно до OpenAI Chat API. Jina AI зазначає, що їхній API “fully compatible with OpenAI’s Chat API schema – просто замініть api.openai.com на deepsearch.jina.ai” ￼. Тобто, клієнтські бібліотеки чи інтеграції, написані під OpenAI, будуть працювати і з Deep Search, достатньо змінити базовий URL та модель.

Приклад запиту: Запит виглядає майже так само, як до ChatGPT. Наприклад, через curl це може бути:

curl https://deepsearch.jina.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <ваш_API_ключ>" \
  -d '{
    "model": "jina-deepsearch-v1",
    "messages": [
       {"role": "user", "content": "Ваш запит..."}
    ],
    "stream": false
  }'

У відповіді Deep Search API поверне об’єкт з полями, подібними до відповіді OpenAI (id, object, choices тощо), але доповнено власними даними. Зокрема, Deep Search може повертати проміжні “розмірковування” моделі (якщо ввімкнено streaming або special tokens <think>...</think>), а також список опрацьованих URL (цитації), звідки взято інформацію ￼ ￼. Завдяки цьому, відповідь містить не лише текстовий висновок, а й посилання на джерела (у форматі Markdown-виносок [^1], [^2] і т.д., як зазначено в гайдах інтеграції ￼).

Документація та корисні ресурси: Офіційна сторінка Deep Search містить інтерактивну документацію. Вона підкреслює ключові параметри:
	•	streaming – режим потокової передачі відповіді (рекомендується ввімкненим, щоб не отримувати таймаут на довгих запитах) ￼;
	•	reasoning_effort – рівень “зусиль” на обмірковування (low, medium, high) для регулювання швидкості vs глибини;
	•	budget_tokens і max_attempts – обмеження на кількість токенів та спроб при вирішенні складних запитів (для контролю витрат і тривалості) ￼ ￼.

Оскільки API повністю сумісний з OpenAI, ви можете використовувати наявні клієнти або ноди n8n, призначені для OpenAI, але вказавши кастомний базовий URL. В контексті n8n, це означає: можна скористатися AI Agent або HTTP Request нодою. Наприклад, у OpenAI Chat Model (як частині AI Agent) передбачена опція кастомної базової адреси – туди можна підставити https://deepsearch.jina.ai/v1 і модель jina-deepsearch-v1 (спільнота вже обговорювала використання власних сумісних API з AI Agent ￼). Таким чином, Deep Search API може слугувати безкоштовною альтернативою OpenAI для сценаріїв, де потрібно автоматично шукати та генерувати звіти (як у шаблоні AI-Powered Research від n8n ￼ ￼).

4. Інтеграція LLM-агента в n8n

Сучасні версії n8n мають розширені можливості для інтеграції моделей штучного інтелекту (LLM) і агентів на їх основі. Під агентом зазвичай розуміється система, що може приймати завдання мовою, самостійно вирішувати, які дії виконати (наприклад, зробити пошук, викликати якусь функцію), і на основі отриманих даних формувати результат. Прикладом є Perplexity AI – сервіс, що приймає запит користувача, шукає інформацію і надає відповідь з цитатами. Нижче розглянемо, як підключити подібні можливості в n8n.

AI Agent node (LangChain Agents в n8n): Починаючи з версій ~1.7x+, n8n інтегрував фреймворк LangChain, що дозволило додати спеціальну ноду AI Agent. Ця нода підтримує кілька типів агентів: Conversational Agent (для ведення діалогу), OpenAI Functions Agent (агент, що може викликати функції через OpenAI Function Calling), Plan and Execute Agent (агент з декомпозицією задачі), ReAct Agent (класичний реагуючий агент), SQL Agent (для роботи з базами даних), Tools Agent (агент, що може користуватись інструментами) та інші ￼. Кожен з цих підходів надає певну стратегію роботи LLM, а вибір залежить від задачі. Наприклад, Conversational Agent дозволяє підтримувати діалог, зберігаючи контекст (в n8n можна додати “пам’ять” – суб-ноду Chat Memory для цього), а Tools Agent може використовувати інші ноди як інструменти (скажімо, ноду веб-пошуку, API-запиту тощо) для знайдення відповіді.

Щоб використати AI Agent у workflow, ви додаєте ноду AI Agent, обираєте тип агента і налаштовуєте підключені LLM-моделі та інструменти. Під капотом, AI Agent складається з головної ноди-агента та набору суб-нод (вони додаються всередині агента). Серед суб-нод є різні провайдери моделей: OpenAI, Azure OpenAI, Anthropic, Google PaLM, Cohere, Local (Ollama, Mistral тощо) – перелік дуже широкий ￼ ￼. Наприклад, щоб агент працював на GPT-4, ви додаєте суб-ноду OpenAI Chat Model та вводите ваш OpenAI API-ключ у її креденшілах. Для використання моделей Azure чи інших – відповідні суб-ноди (Azure OpenAI Chat Model і т.д.). Так само є суб-ноди Tools – наприклад, SerpAPI для пошуку Google, або власні JavaScript-інструменти.

Готові рішення: n8n має галерею шаблонів, де багато прикладів AI-агентів: “AI agent chat”, “AI agent that can scrape webpages”, “Telegram AI bot” тощо ￼ ￼. Наприклад, шаблон AI agent that can scrape webpages демонструє агента, який отримує запит, використовує інструмент “web scrape” і повертає згенеровану відповідь. Є й сценарії, схожі на Perplexity: шаблон “Perplexity Research to HTML” поєднує Perplexity AI для дослідження та GPT-4 для формування контенту ￼ ￼. У цьому шаблоні дані від Perplexity отримуються через її неофіційне API (ймовірно, через HTTP Request до її сервісу), після чого GPT-4 компонует статтю. Тобто, навіть якщо для якогось сервісу (наприклад, Perplexity) немає прямої інтеграції, ви все одно можете викликати його через HTTP, або використати його як інструмент у LangChain-агенті (якщо оформити виклик як Tool).

Прямі запити до LLM без агента: Іноді потрібно просто отримати відповідь від мовної моделі без складної логіки. Для цього можна не використовувати AI Agent, а звернутися до API моделі через стандартні засоби. В n8n є готові інтеграції з OpenAI: зокрема OpenAI Node (в старіших версіях) або ж через HTTP Request. Наприклад, щоб зробити простий запит до ChatGPT, ви можете вручну налаштувати HTTP Request на https://api.openai.com/v1/chat/completions з необхідним JSON (модель, повідомлення) і вказати API-ключ у заголовку Authorization. Але з появою AI Agent більшість таких випадків зручніше реалізувати через нього – достатньо додати Conversational Agent з OpenAI Chat Model всередині. Така нода сама подбає про формат запиту і поверне відповідь як структуровані дані.

Інтеграція власних (нестандартних) LLM: Якщо у вас є свій API сумісний з OpenAI (як у прикладі користувача з локальним LLaMA API ￼), ви можете використати його з AI Agent. Достатньо обрати OpenAI Chat Model і прописати свій Base URL (в налаштуваннях креденшлу OpenAI в n8n можна змінити базовий URL на ваш). При цьому AI Agent буде звертатися до вашого API на /v1/chat/completions як до OpenAI. Спільнота підтвердила, що це працює, потрібно лише врахувати моменти мережі (наприклад, якщо n8n в контейнері Docker, звернутися до хоста через спеціальний hostname) ￼ ￼. Аналогічно, для Deep Search API, як згадано, можна зробити – він OpenAI-сумісний. Таким чином, “будь-який LLM” можна інтегрувати – або через вбудовані конектори n8n, або через універсальний підхід із HTTP API. Як сказано на офіційному сайті n8n, ви можете надавати “AI суперсили” вашим автоматизаціям, використовуючи будь-які LLM і об’єднувати їх з 400+ інтеграціями n8n ￼.

Практичні поради: Після налаштування агента можна підключити його до інтерфейсу, наприклад, використовувати Chat Trigger ноду (текстовий ввод), щоб отримувати питання і передавати агенту ￼. Не забувайте про Memory – n8n дозволяє додати пам’ять (наприклад, Simple Memory або інтеграцію з Redis/Postgres для довготривалої пам’яті) якщо ваш агент має вести бесіду і пам’ятати попередні повідомлення ￼ ￼. Інструменти (Tools) підключаються до Tools Agent або до агентів, що їх підтримують – ви можете написати власний Tool на JavaScript (через суб-ноду Custom Tool), який, наприклад, робить HTTP запит кудись (це аналог того, як Perplexity робить пошук). Завдяки гнучкості n8n, ви можете комбінувати LLM з будь-якими даними: “n8n lets you seamlessly import data from files, websites, or databases into your LLM-powered application and create automated scenarios” ￼. Тобто, агент може отримувати інформацію з інших нод (Google Sheets, БД, API) і використовувати її при формуванні відповіді.

5. Тестування та відлагодження workflow в n8n

Кращі практики для дебагу і логування в n8n:
	1.	Ручне виконання і перевірка даних: У процесі розробки workflow використовуйте можливості редактора n8n для тестування кожної ноди. Ви можете запускати воркфлоу в режимі Manual Execution і поетапно переглядати вихідні дані кожної ноди. n8n відображає результат (вихід) вузла одразу в UI, що дозволяє впевнитися, що нода відпрацювала як очікується. Для тригерних нод (Webhook, Cron тощо) є спеціальний тестовий режим. Наприклад, Webhook-нода має Test URL для відлагодження – ви можете відправити тестовий запит і побачити, як він обробиться, перш ніж активувати workflow ￼ ￼. Рекомендується підставляти прикладні дані і переконуватися, що на виході кожного кроку виходять правильні значення/формат.
	2.	Лог виконань (Executions log): n8n автоматично трекує всі виконання збережених воркфлоу. У лівій панелі інтерфейсу є розділ Executions, де відображається список останніх запусків із статусом (успіх або помилка), часом запуску та тривалістю ￼. Якщо workflow завершився з помилкою, натисніть View (або назву) у списку – відкриється режим перегляду виконання. Ви побачите робочий процес у стані на момент помилки: які ноди відпрацювали, а на якій сталася помилка. Кожна нода показує свій вхід/вихідні дані, і червоний значок помилки на тій, що впала. Такий режим replay у read-only вигляді допомагає точно визначити, на якому кроці і чому сталося виключення ￼ ￼.
	3.	Debug-перезапуск з даними помилкового виконання: Потужна функція n8n – можливість завантажити дані попереднього виконання в редактор для відлагодження. Якщо у вас є невдале виконання, ви можете відкрити його (через Executions -> View), а далі натиснути Debug in editor (для помилкового) або Copy data to editor (для успішного) ￼. Це скопіює всі вхідні дані того запуску в ваш поточний workflow у редакторі і “прикріпить” (pin) їх до початкових нод. Тепер ви можете внести зміни в логіку (виправити помилку) і повторно запустити workflow з тими самими даними, щоб переконатися, що проблема вирішена ￼. Такий цикл значно спрощує відлагодження, особливо для складних сценаріїв, які важко заново відтворити вручну.
	4.	Відловлювання помилок (Error Workflow): У продакшні бажано налаштувати окремий workflow для обробки збоїв – так званий Error Workflow. n8n дозволяє призначити для будь-якого сценарію “воркфлоу помилок” у його налаштуваннях. Цей окремий workflow повинен починатися з ноди Error Trigger і може містити дії сповіщення або логування (наприклад, відправка повідомлення в Slack/Telegram або запис у базу при кожній помилці) ￼ ￼. Коли основний workflow падає, n8n автоматично виконає прив’язаний Error Workflow, передаючи йому інформацію про помилку і контекст. Це дозволяє не пропустити жодного збою і отримати про нього миттєвий сигнал. За замовчуванням, якщо в workflow є нода Error Trigger, то цей же workflow використовується як обробник для себе самого (його не треба активувати, він і так спрацює на помилку) ￼. Практика показує, що використання Error Trigger – найкращий підхід для логування помилок: “ви і так побачите помилку в журналі виконань (Execution log), але можна налаштувати Error Workflow для відправки помилки куди треба… Це кращий підхід, оскільки системні логи n8n більше потрібні для внутрішньої діагностики” ￼ ￼.
	5.	Логування і console.log: При потребі детального логування дій всередині функціональних нод, майте на увазі, що виклик console.log() в Code-ноді (JavaScript) виводить повідомлення не на сервер, а в консоль браузера (в UI) ￼. Тому для “видимих” логів краще явно записувати дані в вихід ноди (наприклад, через return { json: { debugVar: value } } в Code-node), щоб потім бачити їх в Execution log. Або ж використовувати ноди Send to… (наприклад, надсилати проміжні дані собі на email/Slack для моніторингу). Якщо ви self-hosted, можна налаштувати логування виконань у файл або використати сторонні системи (Splunk, CloudWatch) – але це окремі теми. Більшість потреб закривається штатним логом виконань та Error Workflow.
	6.	Налагодження HTTP-запитів: При інтеграції з API особливу увагу звертайте на відповіді HTTP Request. У разі помилки зовнішній сервіс зазвичай повертає код помилки і текст – n8n кине виняток, який ви побачите у деталях помилки вузла (в UI). Повідомлення про помилку HTTP дуже інформативні – містять код і навіть тіло відповіді сервера, що допомагає зрозуміти причину (наприклад, 400 Bad Request із описом, який параметр невірний ￼ ￼). Використовуйте цю інформацію для виправлення запиту. Для тестування API до інтеграції в ноду може бути корисним перевірити запит у сторонньому клієнті (Postman, cURL) – так ви ізолюєте проблему (чи в запиті справа, чи в логіці workflow). Після того, як запит успішно працює у Postman, відтворіть ті ж заголовки і тіло в HTTP Request-ноді.
	7.	“Continue On Fail” та трата помилок: n8n дозволяє налаштувати опцію Continue On Fail для кожної ноди – якщо увімкнути, то навіть при помилці workflow не зупиниться, а перейде далі (а помилковий результат буде в спеціальному полі). Цей режим корисний на етапі відлагодження, коли ви хочете протестувати подальші ноди навіть якщо попередня дала збій. Проте в продакшні краще залишати за замовчуванням (зупинка при помилці), щоб коректно ловити їх Error Workflow’ом.

Дотримуючись цих практик, ви зможете швидко ізолювати і виправити проблеми в ваших сценаріях на n8n. Інструменти n8n для відлагодження – інтуїтивні і потужні: від візуального перегляду історії виконань до гнучкого повторного запуску з наявними даними ￼. А налагоджене логування (через Error Trigger) забезпечить, що жодна помилка не залишиться непоміченою, навіть коли ваші воркфлови працюють автоматично цілодобово.